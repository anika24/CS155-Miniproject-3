{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8WG2gZgsunS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import fileinput\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Activation\n",
        "from keras.layers import Lambda\n",
        "from keras.layers import LSTM\n",
        "from keras.optimizers import RMSprop\n",
        "import numpy as np\n",
        "import random\n",
        "import requests\n",
        "from google.colab import output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndTzQhC4vcis"
      },
      "source": [
        "#Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punct(text):\n",
        "  remove = ',!:;^_`()\"#$\\r\\n123456789'\n",
        "  for chr in remove:\n",
        "    if chr in text:\n",
        "      text = text.replace(chr, ' ')\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "9mbwPfUOXNFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZvebdXCs54r"
      },
      "outputs": [],
      "source": [
        "shakespeare_file = 'https://raw.githubusercontent.com/charlesincharge/Caltech-CS155-2022/main/miniprojects/miniproject3/data/shakespeare.txt'\n",
        "text = requests.get(shakespeare_file).text\n",
        "text = text.lower() # make lowercase\n",
        "text = remove_punct(text) # remove punctuation\n",
        "text = ' '.join(text.split()) # make there a max of one space between words"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate x and y data\n",
        "X_seqs = []\n",
        "Y_chars = []\n",
        "len_seq = 40\n",
        "\n",
        "for i in range(0, len(text)-len_seq, 2):\n",
        "    X_seqs.append(text[i:i+len_seq]) # X: sequences\n",
        "    Y_chars.append(text[i+len_seq])  # Y: chars (the first char after the end of the corresponding seq)"
      ],
      "metadata": {
        "id": "b3VrLB90e2q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate char map\n",
        "chars = sorted(list(set(text)))\n",
        "char_map = {}\n",
        "idx = 0\n",
        "for ch in chars:\n",
        "  char_map[ch] = idx\n",
        "  idx += 1"
      ],
      "metadata": {
        "id": "0wjMM6VvcInL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode a character based on char_map\n",
        "def vectorize_data(char_map, char):\n",
        "  encoded = np.zeros(len(char_map))\n",
        "  encoded[char_map[char]] = 1\n",
        "  return np.array(encoded)"
      ],
      "metadata": {
        "id": "HiMp50bUjL2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# represent each character in x and y by their one hot encoded\n",
        "X_train = np.array([np.array([vectorize_data(char_map, char) for char in X_seqs[i]]) for i in range(len(X_seqs))])\n",
        "Y_train = np.array([vectorize_data(char_map, char) for char in Y_chars])"
      ],
      "metadata": {
        "id": "VwtP6NMaa9YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "XrfCAoa3IgoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(125, input_shape = (40, len(char_map))))\n",
        "model.add(Dense(len(char_map), activation='softmax'))\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWlwNGHjmZOa",
        "outputId": "3e19d7a6-63ca-4d6c-bd93-cb240d70c87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 125)               79000     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                4032      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 83,032\n",
            "Trainable params: 83,032\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = 'RMSprop', loss=\"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
        "model.fit(X_train, Y_train, epochs = 75, batch_size = 128)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18-IVw4lyOzL",
        "outputId": "a08cc717-2d80-43eb-8db3-6343f5331810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "358/358 [==============================] - 52s 135ms/step - loss: 2.7076 - accuracy: 0.2371\n",
            "Epoch 2/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 2.3299 - accuracy: 0.3330\n",
            "Epoch 3/75\n",
            "358/358 [==============================] - 50s 138ms/step - loss: 2.1967 - accuracy: 0.3617\n",
            "Epoch 4/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 2.1233 - accuracy: 0.3781\n",
            "Epoch 5/75\n",
            "358/358 [==============================] - 49s 137ms/step - loss: 2.0693 - accuracy: 0.3905\n",
            "Epoch 6/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 2.0246 - accuracy: 0.4026\n",
            "Epoch 7/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 1.9843 - accuracy: 0.4124\n",
            "Epoch 8/75\n",
            "358/358 [==============================] - 50s 141ms/step - loss: 1.9500 - accuracy: 0.4185\n",
            "Epoch 9/75\n",
            "358/358 [==============================] - 50s 140ms/step - loss: 1.9176 - accuracy: 0.4288\n",
            "Epoch 10/75\n",
            "358/358 [==============================] - 68s 191ms/step - loss: 1.8900 - accuracy: 0.4361\n",
            "Epoch 11/75\n",
            "358/358 [==============================] - 50s 140ms/step - loss: 1.8631 - accuracy: 0.4432\n",
            "Epoch 12/75\n",
            "358/358 [==============================] - 50s 141ms/step - loss: 1.8386 - accuracy: 0.4487\n",
            "Epoch 13/75\n",
            "358/358 [==============================] - 50s 140ms/step - loss: 1.8171 - accuracy: 0.4534\n",
            "Epoch 14/75\n",
            "358/358 [==============================] - 50s 138ms/step - loss: 1.7957 - accuracy: 0.4588\n",
            "Epoch 15/75\n",
            "358/358 [==============================] - 50s 140ms/step - loss: 1.7755 - accuracy: 0.4646\n",
            "Epoch 16/75\n",
            "358/358 [==============================] - 50s 141ms/step - loss: 1.7579 - accuracy: 0.4684\n",
            "Epoch 17/75\n",
            "358/358 [==============================] - 51s 141ms/step - loss: 1.7399 - accuracy: 0.4733\n",
            "Epoch 18/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.7231 - accuracy: 0.4773\n",
            "Epoch 19/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.7066 - accuracy: 0.4821\n",
            "Epoch 20/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 1.6917 - accuracy: 0.4870\n",
            "Epoch 21/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 1.6767 - accuracy: 0.4898\n",
            "Epoch 22/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.6618 - accuracy: 0.4924\n",
            "Epoch 23/75\n",
            "358/358 [==============================] - 50s 140ms/step - loss: 1.6472 - accuracy: 0.4971\n",
            "Epoch 24/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.6339 - accuracy: 0.5006\n",
            "Epoch 25/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.6213 - accuracy: 0.5049\n",
            "Epoch 26/75\n",
            "358/358 [==============================] - 51s 144ms/step - loss: 1.6073 - accuracy: 0.5064\n",
            "Epoch 27/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.5954 - accuracy: 0.5107\n",
            "Epoch 28/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.5820 - accuracy: 0.5141\n",
            "Epoch 29/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.5689 - accuracy: 0.5171\n",
            "Epoch 30/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.5575 - accuracy: 0.5207\n",
            "Epoch 31/75\n",
            "358/358 [==============================] - 50s 141ms/step - loss: 1.5444 - accuracy: 0.5252\n",
            "Epoch 32/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.5313 - accuracy: 0.5290\n",
            "Epoch 33/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.5197 - accuracy: 0.5319\n",
            "Epoch 34/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.5063 - accuracy: 0.5348\n",
            "Epoch 35/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.4940 - accuracy: 0.5398\n",
            "Epoch 36/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.4813 - accuracy: 0.5427\n",
            "Epoch 37/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.4683 - accuracy: 0.5472\n",
            "Epoch 38/75\n",
            "358/358 [==============================] - 51s 144ms/step - loss: 1.4555 - accuracy: 0.5498\n",
            "Epoch 39/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.4424 - accuracy: 0.5538\n",
            "Epoch 40/75\n",
            "358/358 [==============================] - 51s 142ms/step - loss: 1.4290 - accuracy: 0.5577\n",
            "Epoch 41/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.4168 - accuracy: 0.5598\n",
            "Epoch 42/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.4036 - accuracy: 0.5641\n",
            "Epoch 43/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.3899 - accuracy: 0.5681\n",
            "Epoch 44/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.3772 - accuracy: 0.5709\n",
            "Epoch 45/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.3625 - accuracy: 0.5741\n",
            "Epoch 46/75\n",
            "358/358 [==============================] - 52s 146ms/step - loss: 1.3487 - accuracy: 0.5800\n",
            "Epoch 47/75\n",
            "358/358 [==============================] - 52s 146ms/step - loss: 1.3350 - accuracy: 0.5842\n",
            "Epoch 48/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.3219 - accuracy: 0.5895\n",
            "Epoch 49/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.3078 - accuracy: 0.5942\n",
            "Epoch 50/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.2923 - accuracy: 0.5984\n",
            "Epoch 51/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 1.2774 - accuracy: 0.6034\n",
            "Epoch 52/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.2623 - accuracy: 0.6065\n",
            "Epoch 53/75\n",
            "358/358 [==============================] - 51s 144ms/step - loss: 1.2482 - accuracy: 0.6112\n",
            "Epoch 54/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.2327 - accuracy: 0.6165\n",
            "Epoch 55/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.2173 - accuracy: 0.6216\n",
            "Epoch 56/75\n",
            "358/358 [==============================] - 54s 150ms/step - loss: 1.2012 - accuracy: 0.6290\n",
            "Epoch 57/75\n",
            "358/358 [==============================] - 51s 144ms/step - loss: 1.1855 - accuracy: 0.6332\n",
            "Epoch 58/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.1702 - accuracy: 0.6364\n",
            "Epoch 59/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.1535 - accuracy: 0.6438\n",
            "Epoch 60/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.1386 - accuracy: 0.6467\n",
            "Epoch 61/75\n",
            "358/358 [==============================] - 52s 146ms/step - loss: 1.1219 - accuracy: 0.6524\n",
            "Epoch 62/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.1051 - accuracy: 0.6585\n",
            "Epoch 63/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.0903 - accuracy: 0.6637\n",
            "Epoch 64/75\n",
            "358/358 [==============================] - 51s 143ms/step - loss: 1.0736 - accuracy: 0.6687\n",
            "Epoch 65/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 1.0579 - accuracy: 0.6736\n",
            "Epoch 66/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.0422 - accuracy: 0.6779\n",
            "Epoch 67/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.0244 - accuracy: 0.6838\n",
            "Epoch 68/75\n",
            "358/358 [==============================] - 52s 145ms/step - loss: 1.0102 - accuracy: 0.6901\n",
            "Epoch 69/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 0.9926 - accuracy: 0.6941\n",
            "Epoch 70/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 0.9784 - accuracy: 0.6980\n",
            "Epoch 71/75\n",
            "358/358 [==============================] - 52s 144ms/step - loss: 0.9617 - accuracy: 0.7056\n",
            "Epoch 72/75\n",
            "358/358 [==============================] - 52s 146ms/step - loss: 0.9487 - accuracy: 0.7085\n",
            "Epoch 73/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 0.9352 - accuracy: 0.7134\n",
            "Epoch 74/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 0.9212 - accuracy: 0.7183\n",
            "Epoch 75/75\n",
            "358/358 [==============================] - 53s 147ms/step - loss: 0.9056 - accuracy: 0.7227\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f45145c97f0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generate Output"
      ],
      "metadata": {
        "id": "T3w5BEUX9WT2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_char(input, temp):\n",
        "    probs = np.exp(np.log(input) / temp)\n",
        "    probs /= np.sum(probs)\n",
        "    idx = np.random.choice(np.arange(0, len(char_map)), p=probs)\n",
        "    for i in char_map.keys():\n",
        "        if char_map[i] == idx:\n",
        "            return i\n",
        "    return \"\""
      ],
      "metadata": {
        "id": "Sg5ikH17D20s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_sonnet(temp, num_lines, line_len):\n",
        "    sonnet = \"shall i compare thee to a summer's day? \"\n",
        "    for line in range(num_lines * line_len):\n",
        "        layer = np.zeros((1, line_len, len(char_map)))\n",
        "        for i in range(line_len):\n",
        "            char = sonnet[len(sonnet) - line_len + i]\n",
        "            layer[0,i] = vectorize_data(char_map, char)\n",
        "        sonnet += predict_char(model.predict(layer)[0], temp)\n",
        "    return sonnet"
      ],
      "metadata": {
        "id": "qrzGzfY5D9j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_sonnet(txt, num_lines, line_len):\n",
        "    for i in range(1, num_lines * line_len):\n",
        "        if i % line_len == 0:\n",
        "            print(txt[i-1])\n",
        "        else:\n",
        "            print(txt[i-1], end = '')"
      ],
      "metadata": {
        "id": "RebeUMFvR2_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t1 = gen_sonnet(0.25, 14, 40)\n",
        "t2 = gen_sonnet(0.75, 14, 40)\n",
        "t3 = gen_sonnet(1.5, 14, 40)\n",
        "output.clear()"
      ],
      "metadata": {
        "id": "p6Oun2KUENVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sonnet with temp = 0.25')\n",
        "print('----------------------')\n",
        "print_sonnet(t1, 14, 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ci5a279PAyG",
        "outputId": "09023985-506f-4802-bbe6-61ed2544a818"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sonnet with temp = 0.25\n",
            "----------------------\n",
            "shall i compare thee to a summer's day? \n",
            "thou my love thy state of thy shall of m\n",
            "y dost did art and to self thee love the\n",
            " self mine eyes whe eeppeated to my self\n",
            " i am my love and that grain of thy self\n",
            " alons ad to the fearth make thoughts mo\n",
            "rtelled with thengs your seep appert who\n",
            " hast beaven beate. thy should i leave w\n",
            "hine eye hath a see in see in them false\n",
            " as forting of enten and hask the sweet \n",
            "beauty's from mightrow and though should\n",
            " you trought a agat for my love now the \n",
            "dearty might and yet the beauty the worl\n",
            "d will be woos and stating hath to glea"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Sonnet with temp = 0.75')\n",
        "print('----------------------')\n",
        "print_sonnet(t2, 14, 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkxF2cj7PFk0",
        "outputId": "1b9ed20c-927f-431d-e1a4-cb868ca96746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sonnet with temp = 0.75\n",
            "----------------------\n",
            "shall i compare thee to a summer's day? \n",
            "thou art branse o nater doth past steres\n",
            "e not bearows all to chat fan is fair gi\n",
            "ve would tables bright as true thoo shal\n",
            "t her wied sumpary for thy soul foow o n\n",
            "o len accent to leade own reveit tought \n",
            "in a know the gave thou of but hin in th\n",
            "e is self my soull asse inet. both heave\n",
            "nts of eyss wait thou sen-till well of y\n",
            "our self wor well be the death after bin\n",
            "d. so worthy so that so that who herker \n",
            "bettired and beauty and were swares and \n",
            "beauty aboth to my self wearth destake t\n",
            "heir raysoms treesscrengs to purse vink"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('')\n",
        "print('Sonnet with temp = 1.5')\n",
        "print('----------------------')\n",
        "print_sonnet(t3, 14, 40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dt5gmcNuPJpv",
        "outputId": "3f270424-1eca-4a6f-dd84-2f3d18fb94f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sonnet with temp = 1.5\n",
            "----------------------\n",
            "shall i compare thee to a summer's day? \n",
            "and mune all wouthse pyoved tsill of thy\n",
            " heart's frome 'tong. whrich heart. then\n",
            " yet self men. i seent upind mour's had.\n",
            " on wrondleed hasd in impresing thy forr\n",
            "ous offee so iftent with this play far f\n",
            "lom that bebind hay refo but which alavi\n",
            "ng heath deyoiess tingurieden bies chads\n",
            "e ke conit unfrllomst ulfantresed vickun\n",
            "t of a rfifute pispiod od far theil dowi\n",
            "ng that wrut'st an dost fassing arind my\n",
            " love eneing not no to i de this poversu\n",
            "llst wear gzeats both tiless peigeerye. \n",
            "but be apone to pitwance a cishoan conm"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}